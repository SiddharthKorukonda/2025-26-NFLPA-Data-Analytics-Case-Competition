{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2324abe-75ba-48c3-b62f-2977bd4406e6",
   "metadata": {},
   "source": [
    "We initialize Python imports and opens a DuckDB connection that every later cell reuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b58a384e-072d-4d8a-afb8-6663463e37d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DB_PATH /Users/ramko/Desktop/2025-26-NFLPA-Data-Analytics-Case-Competition/db/nflpa.duckdb\n",
      "   rows\n",
      "0  6782\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "root = None\n",
    "for p in [cwd] + list(cwd.parents):\n",
    "    if (p / \"db\").exists():\n",
    "        root = p\n",
    "        break\n",
    "\n",
    "if root is None:\n",
    "    raise FileNotFoundError(\"Could not find a db folder above the current working directory\")\n",
    "\n",
    "DB_PATH = root / \"db\" / \"nflpa.duckdb\"\n",
    "print(\"Using DB_PATH\", DB_PATH)\n",
    "\n",
    "con = duckdb.connect(str(DB_PATH))\n",
    "\n",
    "con.execute(\"PRAGMA threads=4\")\n",
    "con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "\n",
    "print(con.execute(\"SELECT COUNT(*) AS rows FROM team_week_panel\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a850a79-cd91-4464-987d-0a7cc9ca6769",
   "metadata": {},
   "source": [
    "We confirm that the 'team_week_panel' table, which houses our rows keyed by season, week, and team ID, is physically present in the workspace, sets the team and abbreviation columns to match the configuration used for gathering raw data and building the panel, and also loads the necessary utility functions for the PCA preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5119fb5d-e377-4c3b-be9f-dd0d2e0a709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TEAM_COL team\n",
      "Using TEAM_ABBR_COL team\n",
      "True False\n"
     ]
    }
   ],
   "source": [
    "tables = set(con.execute(\"SHOW TABLES\").df()[\"name\"].tolist())\n",
    "\n",
    "if \"team_week_panel\" not in tables:\n",
    "    raise RuntimeError(\"team_week_panel missing, run notebooks 01 through 07 first\")\n",
    "\n",
    "panel_cols = con.execute(\"PRAGMA table_info('team_week_panel')\").df()\n",
    "panel_cols_list = panel_cols[\"name\"].tolist()\n",
    "panel_cols_set = set(panel_cols_list)\n",
    "\n",
    "TEAM_COL = \"team_id\" if \"team_id\" in panel_cols_set else \"team\"\n",
    "TEAM_ABBR_COL = \"team\" if \"team\" in panel_cols_set else TEAM_COL\n",
    "\n",
    "print(\"Using TEAM_COL\", TEAM_COL)\n",
    "print(\"Using TEAM_ABBR_COL\", TEAM_ABBR_COL)\n",
    "\n",
    "def _existing_cols(table_name):\n",
    "    return set(con.execute(f\"PRAGMA table_info('{table_name}')\").df()[\"name\"].tolist())\n",
    "\n",
    "def _star_excluding(table_name, alias, cols_to_maybe_exclude):\n",
    "    existing = _existing_cols(table_name)\n",
    "    keep = [c for c in cols_to_maybe_exclude if c in existing]\n",
    "    if keep:\n",
    "        return f\"{alias}.* EXCLUDE ({', '.join(keep)})\"\n",
    "    return f\"{alias}.*\"\n",
    "\n",
    "print(\"team\" in panel_cols_set, \"team_id\" in panel_cols_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0120b6-a1da-4863-a00f-934cc2e90fc2",
   "metadata": {},
   "source": [
    "We define the exact set of cumulative and per-game workload columns that will feed into the PCA, and also performs a strict validation check against the 'team_week_panel' to ensure all previously persisted data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901c39f2-8d9b-429f-939e-bf0dec97dac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing required inputs []\n",
      "Final PCA input columns used ['cum_off_snaps_w', 'cum_def_snaps_w', 'cum_ST_Load_w', 'cum_short_weeks_w', 'cum_long_travel_w', 'cum_timezone_changes_w', 'cum_west_to_east_w', 'cum_total_snaps_w', 'cum_rest_deficit_days_w', 'cum_away_games_w', 'cum_byes_w']\n"
     ]
    }
   ],
   "source": [
    "required_inputs = [\n",
    "    \"cum_off_snaps_w\",\n",
    "    \"cum_def_snaps_w\",\n",
    "    \"cum_ST_Load_w\",\n",
    "    \"cum_short_weeks_w\",\n",
    "    \"cum_long_travel_w\",\n",
    "    \"cum_timezone_changes_w\",\n",
    "    \"cum_west_to_east_w\",\n",
    "    \"cum_total_snaps_w\",\n",
    "    \"cum_rest_deficit_days_w\",\n",
    "    \"cum_away_games_w\",\n",
    "    \"cum_byes_w\",\n",
    "]\n",
    "\n",
    "cols_now = _existing_cols(\"team_week_panel\")\n",
    "\n",
    "missing_required = [c for c in required_inputs if c not in cols_now]\n",
    "print(\"Missing required inputs\", missing_required)\n",
    "\n",
    "if missing_required:\n",
    "    raise RuntimeError(\n",
    "        \"Step 8 cannot run because required cumulative columns are missing, missing are \"\n",
    "        + \", \".join(missing_required)\n",
    "    )\n",
    "\n",
    "pca_inputs = required_inputs\n",
    "\n",
    "print(\"Final PCA input columns used\", pca_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee426c8-5962-4133-93ee-0960768632c9",
   "metadata": {},
   "source": [
    "We pull the PCA inputs into pandas, verifies that each team-week has a unique index to prevent data leakage, handles any missing values through imputation or zero-filling, and then standardizes each variable across the entire dataset to prepare for the Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f394704-201d-4160-bc09-8145d399065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing rate per input\n",
      "cum_off_snaps_w            0.0\n",
      "cum_def_snaps_w            0.0\n",
      "cum_ST_Load_w              0.0\n",
      "cum_short_weeks_w          0.0\n",
      "cum_long_travel_w          0.0\n",
      "cum_timezone_changes_w     0.0\n",
      "cum_west_to_east_w         0.0\n",
      "cum_total_snaps_w          0.0\n",
      "cum_rest_deficit_days_w    0.0\n",
      "cum_away_games_w           0.0\n",
      "cum_byes_w                 0.0\n",
      "dtype: float64\n",
      "Zero standard deviation inputs dropped []\n",
      "Inputs used after drop ['cum_off_snaps_w', 'cum_def_snaps_w', 'cum_ST_Load_w', 'cum_short_weeks_w', 'cum_long_travel_w', 'cum_timezone_changes_w', 'cum_west_to_east_w', 'cum_total_snaps_w', 'cum_rest_deficit_days_w', 'cum_away_games_w', 'cum_byes_w']\n",
      "Z shape (6782, 11)\n",
      "cum_total_snaps_w         -1.246750e-16\n",
      "cum_ST_Load_w             -8.407705e-17\n",
      "cum_short_weeks_w         -4.295525e-17\n",
      "cum_long_travel_w         -3.771681e-17\n",
      "cum_off_snaps_w           -3.562143e-17\n",
      "cum_west_to_east_w        -2.933529e-17\n",
      "cum_rest_deficit_days_w    1.676303e-17\n",
      "cum_byes_w                 4.190756e-17\n",
      "cum_away_games_w           8.407705e-17\n",
      "cum_timezone_changes_w     1.089597e-16\n",
      "cum_def_snaps_w            1.225796e-16\n",
      "dtype: float64\n",
      "cum_off_snaps_w            1.0\n",
      "cum_def_snaps_w            1.0\n",
      "cum_ST_Load_w              1.0\n",
      "cum_short_weeks_w          1.0\n",
      "cum_long_travel_w          1.0\n",
      "cum_timezone_changes_w     1.0\n",
      "cum_west_to_east_w         1.0\n",
      "cum_total_snaps_w          1.0\n",
      "cum_rest_deficit_days_w    1.0\n",
      "cum_away_games_w           1.0\n",
      "cum_byes_w                 1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = con.execute(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "      season,\n",
    "      week,\n",
    "      {TEAM_ABBR_COL} AS team_key,\n",
    "      {\", \".join(pca_inputs)}\n",
    "    FROM team_week_panel\n",
    "    \"\"\"\n",
    ").df()\n",
    "\n",
    "if df.duplicated(subset=[\"season\", \"week\", \"team_key\"]).any():\n",
    "    n_dup = int(df.duplicated(subset=[\"season\", \"week\", \"team_key\"]).sum())\n",
    "    raise RuntimeError(f\"Duplicate keys found in extracted panel, duplicates {n_dup}\")\n",
    "\n",
    "for c in pca_inputs:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "missing_rates = df[pca_inputs].isna().mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Missing rate per input\")\n",
    "print(missing_rates)\n",
    "\n",
    "df[pca_inputs] = df[pca_inputs].fillna(0.0)\n",
    "\n",
    "means = df[pca_inputs].mean(axis=0)\n",
    "sds = df[pca_inputs].std(axis=0, ddof=0)\n",
    "\n",
    "zero_sd = [c for c in pca_inputs if float(sds[c]) == 0.0 or np.isclose(float(sds[c]), 0.0)]\n",
    "use_inputs = [c for c in pca_inputs if c not in zero_sd]\n",
    "\n",
    "print(\"Zero standard deviation inputs dropped\", zero_sd)\n",
    "print(\"Inputs used after drop\", use_inputs)\n",
    "\n",
    "if len(use_inputs) < 3:\n",
    "    raise RuntimeError(\"Too few usable inputs for PCA after dropping zero variance columns\")\n",
    "\n",
    "Z = (df[use_inputs] - means[use_inputs]) / sds[use_inputs]\n",
    "Z = Z.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "print(\"Z shape\", Z.shape)\n",
    "print(Z.mean().sort_values())\n",
    "print(Z.std(ddof=0).sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330bc112-a318-4107-91ee-de2d97294240",
   "metadata": {},
   "source": [
    "We run PCA on the standardized cumulative and per-game rate inputs using a single component, then extracts the PC1 scores to create our 'Cumulative_Workload_Index_w' while reporting the explained variance ratio to confirm how much information is retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef9810d-b237-42f3-bbf8-66dea05f1139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio PC1 0.6374030207696648\n",
      "Top loadings by absolute value\n",
      "cum_total_snaps_w          0.359774\n",
      "cum_def_snaps_w            0.358970\n",
      "cum_away_games_w           0.358475\n",
      "cum_off_snaps_w            0.358460\n",
      "cum_ST_Load_w              0.354028\n",
      "cum_byes_w                 0.290195\n",
      "cum_timezone_changes_w     0.285650\n",
      "cum_rest_deficit_days_w    0.276442\n",
      "cum_short_weeks_w          0.273364\n",
      "cum_long_travel_w          0.181837\n",
      "cum_west_to_east_w         0.096682\n",
      "Name: loading_pc1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1, svd_solver=\"full\")\n",
    "pc1_scores = pca.fit_transform(Z.values).reshape(-1)\n",
    "\n",
    "explained_var_ratio_pc1 = float(pca.explained_variance_ratio_[0])\n",
    "\n",
    "loadings = pd.Series(pca.components_[0], index=use_inputs, name=\"loading_pc1\")\n",
    "\n",
    "print(\"Explained variance ratio PC1\", explained_var_ratio_pc1)\n",
    "print(\"Top loadings by absolute value\")\n",
    "print(loadings.reindex(loadings.abs().sort_values(ascending=False).head(15).index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83c92c-086e-4df5-9fa7-63ed4dd812bd",
   "metadata": {},
   "source": [
    "We orient the first principal component so that higher values consistently represent higher cumulative workload, and also flips the sign if PC1 is negatively correlated with total snaps to ensure the index is logically interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a21610-15bb-4086-ad1d-7b204de101c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with cum_total_snaps_w before flip 0.9526503824732212\n",
      "Flip applied 1.0\n",
      "Correlation with cum_total_snaps_w after flip 0.9526503824732212\n"
     ]
    }
   ],
   "source": [
    "anchor = df[\"cum_total_snaps_w\"].values.astype(float)\n",
    "corr_before = float(np.corrcoef(pc1_scores, anchor)[0, 1])\n",
    "\n",
    "flip = 1.0\n",
    "if np.isfinite(corr_before) and corr_before < 0:\n",
    "    flip = -1.0\n",
    "\n",
    "pc1_scores_oriented = pc1_scores * flip\n",
    "loadings_oriented = loadings * flip\n",
    "\n",
    "corr_after = float(np.corrcoef(pc1_scores_oriented, anchor)[0, 1])\n",
    "\n",
    "print(\"Correlation with cum_total_snaps_w before flip\", corr_before)\n",
    "print(\"Flip applied\", flip)\n",
    "print(\"Correlation with cum_total_snaps_w after flip\", corr_after)\n",
    "\n",
    "df_index = df[[\"season\", \"week\", \"team_key\"]].copy()\n",
    "df_index[\"Cumulative_Workload_Index_w\"] = pc1_scores_oriented.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3cf3b-8719-483a-bbb9-87bebcf1e426",
   "metadata": {},
   "source": [
    "We archive the transformation weights and scaling factors to ensure the research is reproducible, while documenting the explained variance and orientation of the first principal component for long-term auditability within the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21b2714-b776-41c4-8000-6925fb98784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               version  n_rows  n_inputs_requested  n_inputs_used  \\\n",
      "0  20260103_212306_utc    6782                  11             11   \n",
      "1  20260103_210903_utc    6782                  11             11   \n",
      "2  20260103_210809_utc    6782                  11             11   \n",
      "\n",
      "   explained_variance_ratio_pc1  pc1_flip                       created_utc  \n",
      "0                      0.637403       1.0  2026-01-03T21:23:06.079576+00:00  \n",
      "1                      0.637403       1.0        2026-01-03T21:09:03.020411  \n",
      "2                      0.637403       1.0  2026-01-03T21:08:09.869236+00:00  \n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "now_utc = dt.datetime.now(dt.UTC)\n",
    "version = now_utc.strftime(\"%Y%m%d_%H%M%S_utc\")\n",
    "created_utc = now_utc.isoformat()\n",
    "\n",
    "artifacts = pd.DataFrame({\n",
    "    \"version\": version,\n",
    "    \"column_name\": use_inputs,\n",
    "    \"mean\": means[use_inputs].values.astype(float),\n",
    "    \"sd\": sds[use_inputs].values.astype(float),\n",
    "    \"loading_pc1\": loadings_oriented.loc[use_inputs].values.astype(float),\n",
    "})\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"version\": version,\n",
    "    \"n_rows\": int(len(df)),\n",
    "    \"n_inputs_requested\": int(len(pca_inputs)),\n",
    "    \"n_inputs_used\": int(len(use_inputs)),\n",
    "    \"explained_variance_ratio_pc1\": float(explained_var_ratio_pc1),\n",
    "    \"pc1_flip\": float(flip),\n",
    "    \"created_utc\": created_utc,\n",
    "}])\n",
    "\n",
    "con.register(\"pca_artifacts_df\", artifacts)\n",
    "con.register(\"pca_summary_df\", summary)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pca_cum_workload_artifacts (\n",
    "  version VARCHAR,\n",
    "  column_name VARCHAR,\n",
    "  mean DOUBLE,\n",
    "  sd DOUBLE,\n",
    "  loading_pc1 DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pca_cum_workload_summary (\n",
    "  version VARCHAR,\n",
    "  n_rows BIGINT,\n",
    "  n_inputs_requested BIGINT,\n",
    "  n_inputs_used BIGINT,\n",
    "  explained_variance_ratio_pc1 DOUBLE,\n",
    "  pc1_flip DOUBLE,\n",
    "  created_utc VARCHAR\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"INSERT INTO pca_cum_workload_artifacts SELECT * FROM pca_artifacts_df\")\n",
    "con.execute(\"INSERT INTO pca_cum_workload_summary SELECT * FROM pca_summary_df\")\n",
    "\n",
    "print(con.execute(\"SELECT * FROM pca_cum_workload_summary ORDER BY created_utc DESC LIMIT 3\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9e308-bce3-41c5-bda7-951493ec45fc",
   "metadata": {},
   "source": [
    "We join our newly derived PCA scores onto the master panel using the team and week identifiers, followed by a density check to confirm that the integration is 100% complete across the entire longitudinal series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e097642-09c4-4ed6-b480-95644b587f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before 6782\n",
      "Rows after 6782\n",
      "Null index rows 0.0\n"
     ]
    }
   ],
   "source": [
    "df_index_for_db = df_index.rename(columns={\"team_key\": TEAM_ABBR_COL})\n",
    "con.register(\"pca_index_df\", df_index_for_db)\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS pca_cum_workload_index_tmp\")\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE pca_cum_workload_index_tmp AS\n",
    "SELECT * FROM pca_index_df\n",
    "\"\"\")\n",
    "\n",
    "pre_rows = int(con.execute(\"SELECT COUNT(*) AS n FROM team_week_panel\").df()[\"n\"].iloc[0])\n",
    "\n",
    "star = _star_excluding(\"team_week_panel\", \"p\", [\"Cumulative_Workload_Index_w\"])\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE team_week_panel AS\n",
    "SELECT\n",
    "  {star},\n",
    "  i.Cumulative_Workload_Index_w\n",
    "FROM team_week_panel p\n",
    "LEFT JOIN pca_cum_workload_index_tmp i\n",
    "USING (season, week, {TEAM_ABBR_COL})\n",
    "\"\"\")\n",
    "\n",
    "post_rows = int(con.execute(\"SELECT COUNT(*) AS n FROM team_week_panel\").df()[\"n\"].iloc[0])\n",
    "\n",
    "nulls = con.execute(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN Cumulative_Workload_Index_w IS NULL THEN 1 ELSE 0 END) AS n_null\n",
    "FROM team_week_panel\n",
    "\"\"\").df()[\"n_null\"].iloc[0]\n",
    "\n",
    "print(\"Rows before\", pre_rows)\n",
    "print(\"Rows after\", post_rows)\n",
    "print(\"Null index rows\", nulls)\n",
    "\n",
    "if pre_rows != post_rows:\n",
    "    raise RuntimeError(\"Row count changed after adding the index, investigate key duplication or join mismatch\")\n",
    "\n",
    "if nulls != 0:\n",
    "    raise RuntimeError(\"Index has nulls after join, investigate missing keys in pca_index_df\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
