{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2324abe-75ba-48c3-b62f-2977bd4406e6",
   "metadata": {},
   "source": [
    "We initialize Python imports and opens a DuckDB connection that every later cell reuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a384e-072d-4d8a-afb8-6663463e37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "root = None\n",
    "for p in [cwd] + list(cwd.parents):\n",
    "    if (p / \"db\").exists():\n",
    "        root = p\n",
    "        break\n",
    "\n",
    "if root is None:\n",
    "    raise FileNotFoundError(\"Could not find a db folder above the current working directory\")\n",
    "\n",
    "DB_PATH = root / \"db\" / \"nflpa.duckdb\"\n",
    "print(\"Using DB_PATH\", DB_PATH)\n",
    "\n",
    "con = duckdb.connect(str(DB_PATH))\n",
    "\n",
    "con.execute(\"PRAGMA threads=4\")\n",
    "con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "\n",
    "print(con.execute(\"SELECT COUNT(*) AS rows FROM team_week_panel\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a850a79-cd91-4464-987d-0a7cc9ca6769",
   "metadata": {},
   "source": [
    "We confirm that the 'team_week_panel' table, which houses our rows keyed by season, week, and team ID, is physically present in the workspace, sets the team and abbreviation columns to match the configuration used for gathering raw data and building the panel, and also loads the necessary utility functions for the PCA preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119fb5d-e377-4c3b-be9f-dd0d2e0a709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = set(con.execute(\"SHOW TABLES\").df()[\"name\"].tolist())\n",
    "\n",
    "if \"team_week_panel\" not in tables:\n",
    "    raise RuntimeError(\"team_week_panel missing, run notebooks 01 through 07 first\")\n",
    "\n",
    "panel_cols = con.execute(\"PRAGMA table_info('team_week_panel')\").df()\n",
    "panel_cols_list = panel_cols[\"name\"].tolist()\n",
    "panel_cols_set = set(panel_cols_list)\n",
    "\n",
    "TEAM_COL = \"team_id\" if \"team_id\" in panel_cols_set else \"team\"\n",
    "TEAM_ABBR_COL = \"team\" if \"team\" in panel_cols_set else TEAM_COL\n",
    "\n",
    "print(\"Using TEAM_COL\", TEAM_COL)\n",
    "print(\"Using TEAM_ABBR_COL\", TEAM_ABBR_COL)\n",
    "\n",
    "def _existing_cols(table_name):\n",
    "    return set(con.execute(f\"PRAGMA table_info('{table_name}')\").df()[\"name\"].tolist())\n",
    "\n",
    "def _star_excluding(table_name, alias, cols_to_maybe_exclude):\n",
    "    existing = _existing_cols(table_name)\n",
    "    keep = [c for c in cols_to_maybe_exclude if c in existing]\n",
    "    if keep:\n",
    "        return f\"{alias}.* EXCLUDE ({', '.join(keep)})\"\n",
    "    return f\"{alias}.*\"\n",
    "\n",
    "print(\"team\" in panel_cols_set, \"team_id\" in panel_cols_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0120b6-a1da-4863-a00f-934cc2e90fc2",
   "metadata": {},
   "source": [
    "We define the exact set of cumulative and per-game workload columns that will feed into the PCA, and also performs a strict validation check against the 'team_week_panel' to ensure all previously persisted data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c39f2-8d9b-429f-939e-bf0dec97dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_inputs = [\n",
    "    \"cum_off_snaps_w\",\n",
    "    \"cum_def_snaps_w\",\n",
    "    \"cum_ST_Load_w\",\n",
    "    \"cum_short_weeks_w\",\n",
    "    \"cum_long_travel_w\",\n",
    "    \"cum_timezone_changes_w\",\n",
    "    \"cum_west_to_east_w\",\n",
    "    \"cum_total_snaps_w\",\n",
    "    \"cum_rest_deficit_days_w\",\n",
    "    \"cum_away_games_w\",\n",
    "    \"cum_byes_w\",\n",
    "]\n",
    "\n",
    "cols_now = _existing_cols(\"team_week_panel\")\n",
    "\n",
    "missing_required = [c for c in required_inputs if c not in cols_now]\n",
    "print(\"Missing required inputs\", missing_required)\n",
    "\n",
    "if missing_required:\n",
    "    raise RuntimeError(\n",
    "        \"Step 8 cannot run because required cumulative columns are missing, missing are \"\n",
    "        + \", \".join(missing_required)\n",
    "    )\n",
    "\n",
    "pca_inputs = required_inputs\n",
    "\n",
    "print(\"Final PCA input columns used\", pca_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee426c8-5962-4133-93ee-0960768632c9",
   "metadata": {},
   "source": [
    "We pull the PCA inputs into pandas, verifies that each team-week has a unique index to prevent data leakage, handles any missing values through imputation or zero-filling, and then standardizes each variable across the entire dataset to prepare for the Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f394704-201d-4160-bc09-8145d399065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = con.execute(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "      season,\n",
    "      week,\n",
    "      {TEAM_ABBR_COL} AS team_key,\n",
    "      {\", \".join(pca_inputs)}\n",
    "    FROM team_week_panel\n",
    "    \"\"\"\n",
    ").df()\n",
    "\n",
    "if df.duplicated(subset=[\"season\", \"week\", \"team_key\"]).any():\n",
    "    n_dup = int(df.duplicated(subset=[\"season\", \"week\", \"team_key\"]).sum())\n",
    "    raise RuntimeError(f\"Duplicate keys found in extracted panel, duplicates {n_dup}\")\n",
    "\n",
    "for c in pca_inputs:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "missing_rates = df[pca_inputs].isna().mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Missing rate per input\")\n",
    "print(missing_rates)\n",
    "\n",
    "df[pca_inputs] = df[pca_inputs].fillna(0.0)\n",
    "\n",
    "means = df[pca_inputs].mean(axis=0)\n",
    "sds = df[pca_inputs].std(axis=0, ddof=0)\n",
    "\n",
    "zero_sd = [c for c in pca_inputs if float(sds[c]) == 0.0 or np.isclose(float(sds[c]), 0.0)]\n",
    "use_inputs = [c for c in pca_inputs if c not in zero_sd]\n",
    "\n",
    "print(\"Zero standard deviation inputs dropped\", zero_sd)\n",
    "print(\"Inputs used after drop\", use_inputs)\n",
    "\n",
    "if len(use_inputs) < 3:\n",
    "    raise RuntimeError(\"Too few usable inputs for PCA after dropping zero variance columns\")\n",
    "\n",
    "Z = (df[use_inputs] - means[use_inputs]) / sds[use_inputs]\n",
    "Z = Z.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "print(\"Z shape\", Z.shape)\n",
    "print(Z.mean().sort_values())\n",
    "print(Z.std(ddof=0).sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330bc112-a318-4107-91ee-de2d97294240",
   "metadata": {},
   "source": [
    "We run PCA on the standardized cumulative and per-game rate inputs using a single component, then extracts the PC1 scores to create our 'Cumulative_Workload_Index_w' while reporting the explained variance ratio to confirm how much information is retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9810d-b237-42f3-bbf8-66dea05f1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1, svd_solver=\"full\")\n",
    "pc1_scores = pca.fit_transform(Z.values).reshape(-1)\n",
    "\n",
    "explained_var_ratio_pc1 = float(pca.explained_variance_ratio_[0])\n",
    "\n",
    "loadings = pd.Series(pca.components_[0], index=use_inputs, name=\"loading_pc1\")\n",
    "\n",
    "print(\"Explained variance ratio PC1\", explained_var_ratio_pc1)\n",
    "print(\"Top loadings by absolute value\")\n",
    "print(loadings.reindex(loadings.abs().sort_values(ascending=False).head(15).index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83c92c-086e-4df5-9fa7-63ed4dd812bd",
   "metadata": {},
   "source": [
    "We orient the first principal component so that higher values consistently represent higher cumulative workload, and also flips the sign if PC1 is negatively correlated with total snaps to ensure the index is logically interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a21610-15bb-4086-ad1d-7b204de101c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = df[\"cum_total_snaps_w\"].values.astype(float)\n",
    "corr_before = float(np.corrcoef(pc1_scores, anchor)[0, 1])\n",
    "\n",
    "flip = 1.0\n",
    "if np.isfinite(corr_before) and corr_before < 0:\n",
    "    flip = -1.0\n",
    "\n",
    "pc1_scores_oriented = pc1_scores * flip\n",
    "loadings_oriented = loadings * flip\n",
    "\n",
    "corr_after = float(np.corrcoef(pc1_scores_oriented, anchor)[0, 1])\n",
    "\n",
    "print(\"Correlation with cum_total_snaps_w before flip\", corr_before)\n",
    "print(\"Flip applied\", flip)\n",
    "print(\"Correlation with cum_total_snaps_w after flip\", corr_after)\n",
    "\n",
    "df_index = df[[\"season\", \"week\", \"team_key\"]].copy()\n",
    "df_index[\"Cumulative_Workload_Index_w\"] = pc1_scores_oriented.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3cf3b-8719-483a-bbb9-87bebcf1e426",
   "metadata": {},
   "source": [
    "We archive the transformation weights and scaling factors to ensure the research is reproducible, while documenting the explained variance and orientation of the first principal component for long-term auditability within the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b2714-b776-41c4-8000-6925fb98784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "now_utc = dt.datetime.now(dt.UTC)\n",
    "version = now_utc.strftime(\"%Y%m%d_%H%M%S_utc\")\n",
    "created_utc = now_utc.isoformat()\n",
    "\n",
    "artifacts = pd.DataFrame({\n",
    "    \"version\": version,\n",
    "    \"column_name\": use_inputs,\n",
    "    \"mean\": means[use_inputs].values.astype(float),\n",
    "    \"sd\": sds[use_inputs].values.astype(float),\n",
    "    \"loading_pc1\": loadings_oriented.loc[use_inputs].values.astype(float),\n",
    "})\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"version\": version,\n",
    "    \"n_rows\": int(len(df)),\n",
    "    \"n_inputs_requested\": int(len(pca_inputs)),\n",
    "    \"n_inputs_used\": int(len(use_inputs)),\n",
    "    \"explained_variance_ratio_pc1\": float(explained_var_ratio_pc1),\n",
    "    \"pc1_flip\": float(flip),\n",
    "    \"created_utc\": created_utc,\n",
    "}])\n",
    "\n",
    "con.register(\"pca_artifacts_df\", artifacts)\n",
    "con.register(\"pca_summary_df\", summary)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pca_cum_workload_artifacts (\n",
    "  version VARCHAR,\n",
    "  column_name VARCHAR,\n",
    "  mean DOUBLE,\n",
    "  sd DOUBLE,\n",
    "  loading_pc1 DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pca_cum_workload_summary (\n",
    "  version VARCHAR,\n",
    "  n_rows BIGINT,\n",
    "  n_inputs_requested BIGINT,\n",
    "  n_inputs_used BIGINT,\n",
    "  explained_variance_ratio_pc1 DOUBLE,\n",
    "  pc1_flip DOUBLE,\n",
    "  created_utc VARCHAR\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"INSERT INTO pca_cum_workload_artifacts SELECT * FROM pca_artifacts_df\")\n",
    "con.execute(\"INSERT INTO pca_cum_workload_summary SELECT * FROM pca_summary_df\")\n",
    "\n",
    "print(con.execute(\"SELECT * FROM pca_cum_workload_summary ORDER BY created_utc DESC LIMIT 3\").df())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
