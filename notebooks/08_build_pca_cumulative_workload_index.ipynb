{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2324abe-75ba-48c3-b62f-2977bd4406e6",
   "metadata": {},
   "source": [
    "We initialize Python imports and opens a DuckDB connection that every later cell reuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a384e-072d-4d8a-afb8-6663463e37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "root = None\n",
    "for p in [cwd] + list(cwd.parents):\n",
    "    if (p / \"db\").exists():\n",
    "        root = p\n",
    "        break\n",
    "\n",
    "if root is None:\n",
    "    raise FileNotFoundError(\"Could not find a db folder above the current working directory\")\n",
    "\n",
    "DB_PATH = root / \"db\" / \"nflpa.duckdb\"\n",
    "print(\"Using DB_PATH\", DB_PATH)\n",
    "\n",
    "con = duckdb.connect(str(DB_PATH))\n",
    "\n",
    "con.execute(\"PRAGMA threads=4\")\n",
    "con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "\n",
    "print(con.execute(\"SELECT COUNT(*) AS rows FROM team_week_panel\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a850a79-cd91-4464-987d-0a7cc9ca6769",
   "metadata": {},
   "source": [
    "We confirm that the 'team_week_panel' table, which houses our rows keyed by season, week, and team ID, is physically present in the workspace, sets the team and abbreviation columns to match the configuration used for gathering raw data and building the panel, and also loads the necessary utility functions for the PCA preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119fb5d-e377-4c3b-be9f-dd0d2e0a709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = set(con.execute(\"SHOW TABLES\").df()[\"name\"].tolist())\n",
    "\n",
    "if \"team_week_panel\" not in tables:\n",
    "    raise RuntimeError(\"team_week_panel missing, run notebooks 01 through 07 first\")\n",
    "\n",
    "panel_cols = con.execute(\"PRAGMA table_info('team_week_panel')\").df()\n",
    "panel_cols_list = panel_cols[\"name\"].tolist()\n",
    "panel_cols_set = set(panel_cols_list)\n",
    "\n",
    "TEAM_COL = \"team_id\" if \"team_id\" in panel_cols_set else \"team\"\n",
    "TEAM_ABBR_COL = \"team\" if \"team\" in panel_cols_set else TEAM_COL\n",
    "\n",
    "print(\"Using TEAM_COL\", TEAM_COL)\n",
    "print(\"Using TEAM_ABBR_COL\", TEAM_ABBR_COL)\n",
    "\n",
    "def _existing_cols(table_name):\n",
    "    return set(con.execute(f\"PRAGMA table_info('{table_name}')\").df()[\"name\"].tolist())\n",
    "\n",
    "def _star_excluding(table_name, alias, cols_to_maybe_exclude):\n",
    "    existing = _existing_cols(table_name)\n",
    "    keep = [c for c in cols_to_maybe_exclude if c in existing]\n",
    "    if keep:\n",
    "        return f\"{alias}.* EXCLUDE ({', '.join(keep)})\"\n",
    "    return f\"{alias}.*\"\n",
    "\n",
    "print(\"team\" in panel_cols_set, \"team_id\" in panel_cols_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0120b6-a1da-4863-a00f-934cc2e90fc2",
   "metadata": {},
   "source": [
    "We define the exact set of cumulative and per-game workload columns that will feed into the PCA, and also performs a strict validation check against the 'team_week_panel' to ensure all previously persisted data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c39f2-8d9b-429f-939e-bf0dec97dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_inputs = [\n",
    "    \"cum_off_snaps_w\",\n",
    "    \"cum_def_snaps_w\",\n",
    "    \"cum_ST_Load_w\",\n",
    "    \"cum_short_weeks_w\",\n",
    "    \"cum_long_travel_w\",\n",
    "    \"cum_timezone_changes_w\",\n",
    "    \"cum_west_to_east_w\",\n",
    "    \"cum_total_snaps_w\",\n",
    "    \"cum_rest_deficit_days_w\",\n",
    "    \"cum_away_games_w\",\n",
    "    \"cum_byes_w\",\n",
    "]\n",
    "\n",
    "cols_now = _existing_cols(\"team_week_panel\")\n",
    "\n",
    "missing_required = [c for c in required_inputs if c not in cols_now]\n",
    "print(\"Missing required inputs\", missing_required)\n",
    "\n",
    "if missing_required:\n",
    "    raise RuntimeError(\n",
    "        \"Step 8 cannot run because required cumulative columns are missing, missing are \"\n",
    "        + \", \".join(missing_required)\n",
    "    )\n",
    "\n",
    "pca_inputs = required_inputs\n",
    "\n",
    "print(\"Final PCA input columns used\", pca_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee426c8-5962-4133-93ee-0960768632c9",
   "metadata": {},
   "source": [
    "We pull the PCA inputs into pandas, verifies that each team-week has a unique index to prevent data leakage, handles any missing values through imputation or zero-filling, and then standardizes each variable across the entire dataset to prepare for the Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f394704-201d-4160-bc09-8145d399065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = con.execute(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "      season,\n",
    "      week,\n",
    "      {TEAM_ABBR_COL} AS team_key,\n",
    "      {\", \".join(pca_inputs)}\n",
    "    FROM team_week_panel\n",
    "    \"\"\"\n",
    ").df()\n",
    "\n",
    "if df.duplicated(subset=[\"season\", \"week\", \"team_key\"]).any():\n",
    "    n_dup = int(df.duplicated(subset=[\"season\", \"week\", \"team_key\"]).sum())\n",
    "    raise RuntimeError(f\"Duplicate keys found in extracted panel, duplicates {n_dup}\")\n",
    "\n",
    "for c in pca_inputs:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "missing_rates = df[pca_inputs].isna().mean().sort_values(ascending=False)\n",
    "print(\"Missing rate per input\")\n",
    "print(missing_rates)\n",
    "\n",
    "if float(missing_rates.max()) > 0.0:\n",
    "    bad = missing_rates[missing_rates > 0.0].index.tolist()\n",
    "    raise RuntimeError(\n",
    "        \"Step 8 cannot proceed because PCA inputs contain nulls, null columns are \" + \", \".join(bad)\n",
    "    )\n",
    "\n",
    "means = df[pca_inputs].mean(axis=0)\n",
    "sds = df[pca_inputs].std(axis=0, ddof=0)\n",
    "\n",
    "zero_sd = [c for c in pca_inputs if float(sds[c]) == 0.0 or np.isclose(float(sds[c]), 0.0)]\n",
    "use_inputs = [c for c in pca_inputs if c not in zero_sd]\n",
    "\n",
    "print(\"Zero standard deviation inputs dropped\", zero_sd)\n",
    "print(\"Inputs used after drop\", use_inputs)\n",
    "\n",
    "if len(use_inputs) < 3:\n",
    "    raise RuntimeError(\"Too few usable inputs for PCA after dropping zero variance columns\")\n",
    "\n",
    "Z = (df[use_inputs] - means[use_inputs]) / sds[use_inputs]\n",
    "Z = Z.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "print(\"Z shape\", Z.shape)\n",
    "print(Z.mean().sort_values())\n",
    "print(Z.std(ddof=0).sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330bc112-a318-4107-91ee-de2d97294240",
   "metadata": {},
   "source": [
    "We run PCA on the standardized cumulative and per-game rate inputs using a single component, then extracts the PC1 scores to create our 'Cumulative_Workload_Index_w' while reporting the explained variance ratio to confirm how much information is retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9810d-b237-42f3-bbf8-66dea05f1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1, svd_solver=\"full\")\n",
    "pc1_scores = pca.fit_transform(Z.values).reshape(-1)\n",
    "\n",
    "explained_var_ratio_pc1 = float(pca.explained_variance_ratio_[0])\n",
    "\n",
    "loadings = pd.Series(pca.components_[0], index=use_inputs, name=\"loading_pc1\")\n",
    "\n",
    "print(\"Explained variance ratio PC1\", explained_var_ratio_pc1)\n",
    "print(\"Top loadings by absolute value\")\n",
    "print(loadings.reindex(loadings.abs().sort_values(ascending=False).head(15).index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83c92c-086e-4df5-9fa7-63ed4dd812bd",
   "metadata": {},
   "source": [
    "We orient the first principal component so that higher values consistently represent higher cumulative workload, and also flips the sign if PC1 is negatively correlated with total snaps to ensure the index is logically interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a21610-15bb-4086-ad1d-7b204de101c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = df[\"cum_total_snaps_w\"].values.astype(float)\n",
    "corr_before = float(np.corrcoef(pc1_scores, anchor)[0, 1])\n",
    "\n",
    "flip = 1.0\n",
    "if np.isfinite(corr_before) and corr_before < 0:\n",
    "    flip = -1.0\n",
    "\n",
    "pc1_scores_oriented = pc1_scores * flip\n",
    "loadings_oriented = loadings * flip\n",
    "\n",
    "corr_after = float(np.corrcoef(pc1_scores_oriented, anchor)[0, 1])\n",
    "\n",
    "print(\"Correlation with cum_total_snaps_w before flip\", corr_before)\n",
    "print(\"Flip applied\", flip)\n",
    "print(\"Correlation with cum_total_snaps_w after flip\", corr_after)\n",
    "\n",
    "df_index = df[[\"season\", \"week\", \"team_key\"]].copy()\n",
    "df_index[\"Cumulative_Workload_Index_w\"] = pc1_scores_oriented.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3cf3b-8719-483a-bbb9-87bebcf1e426",
   "metadata": {},
   "source": [
    "We archive the transformation weights and scaling factors to ensure the research is reproducible, while documenting the explained variance and orientation of the first principal component for long-term auditability within the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b2714-b776-41c4-8000-6925fb98784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "now_utc = dt.datetime.now(dt.UTC)\n",
    "version = now_utc.strftime(\"%Y%m%d_%H%M%S_%f_utc\")\n",
    "created_utc = now_utc.isoformat()\n",
    "\n",
    "artifacts = pd.DataFrame({\n",
    "    \"version\": version,\n",
    "    \"column_name\": use_inputs,\n",
    "    \"mean\": means[use_inputs].values.astype(float),\n",
    "    \"sd\": sds[use_inputs].values.astype(float),\n",
    "    \"loading_pc1\": loadings_oriented.loc[use_inputs].values.astype(float),\n",
    "})\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"version\": version,\n",
    "    \"n_rows\": int(len(df)),\n",
    "    \"n_inputs_requested\": int(len(pca_inputs)),\n",
    "    \"n_inputs_used\": int(len(use_inputs)),\n",
    "    \"explained_variance_ratio_pc1\": float(explained_var_ratio_pc1),\n",
    "    \"pc1_flip\": float(flip),\n",
    "    \"created_utc\": created_utc,\n",
    "}])\n",
    "\n",
    "con.register(\"pca_artifacts_df\", artifacts)\n",
    "con.register(\"pca_summary_df\", summary)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pca_cum_workload_artifacts (\n",
    "  version VARCHAR,\n",
    "  column_name VARCHAR,\n",
    "  mean DOUBLE,\n",
    "  sd DOUBLE,\n",
    "  loading_pc1 DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pca_cum_workload_summary (\n",
    "  version VARCHAR,\n",
    "  n_rows BIGINT,\n",
    "  n_inputs_requested BIGINT,\n",
    "  n_inputs_used BIGINT,\n",
    "  explained_variance_ratio_pc1 DOUBLE,\n",
    "  pc1_flip DOUBLE,\n",
    "  created_utc VARCHAR\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"DELETE FROM pca_cum_workload_artifacts WHERE version = ?\", [version])\n",
    "con.execute(\"DELETE FROM pca_cum_workload_summary WHERE version = ?\", [version])\n",
    "\n",
    "con.execute(\"INSERT INTO pca_cum_workload_artifacts SELECT * FROM pca_artifacts_df\")\n",
    "con.execute(\"INSERT INTO pca_cum_workload_summary SELECT * FROM pca_summary_df\")\n",
    "\n",
    "print(con.execute(\"SELECT * FROM pca_cum_workload_summary ORDER BY created_utc DESC LIMIT 3\").df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9e308-bce3-41c5-bda7-951493ec45fc",
   "metadata": {},
   "source": [
    "We join our newly derived PCA scores onto the master panel using the team and week identifiers, followed by a density check to confirm that the integration is 100% complete across the entire longitudinal series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e097642-09c4-4ed6-b480-95644b587f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_for_db = df_index.rename(columns={\"team_key\": TEAM_ABBR_COL})\n",
    "con.register(\"pca_index_df\", df_index_for_db)\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS pca_cum_workload_index_tmp\")\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE pca_cum_workload_index_tmp AS\n",
    "SELECT * FROM pca_index_df\n",
    "\"\"\")\n",
    "\n",
    "pre_rows = int(con.execute(\"SELECT COUNT(*) AS n FROM team_week_panel\").df()[\"n\"].iloc[0])\n",
    "\n",
    "star = _star_excluding(\"team_week_panel\", \"p\", [\"Cumulative_Workload_Index_w\"])\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE team_week_panel AS\n",
    "SELECT\n",
    "  {star},\n",
    "  i.Cumulative_Workload_Index_w\n",
    "FROM team_week_panel p\n",
    "LEFT JOIN pca_cum_workload_index_tmp i\n",
    "USING (season, week, {TEAM_ABBR_COL})\n",
    "\"\"\")\n",
    "\n",
    "post_rows = int(con.execute(\"SELECT COUNT(*) AS n FROM team_week_panel\").df()[\"n\"].iloc[0])\n",
    "\n",
    "nulls = con.execute(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN Cumulative_Workload_Index_w IS NULL THEN 1 ELSE 0 END) AS n_null\n",
    "FROM team_week_panel\n",
    "\"\"\").df()[\"n_null\"].iloc[0]\n",
    "\n",
    "print(\"Rows before\", pre_rows)\n",
    "print(\"Rows after\", post_rows)\n",
    "print(\"Null index rows\", nulls)\n",
    "\n",
    "if pre_rows != post_rows:\n",
    "    raise RuntimeError(\"Row count changed after adding the index, investigate key duplication or join mismatch\")\n",
    "\n",
    "if nulls != 0:\n",
    "    raise RuntimeError(\"Index has nulls after join, investigate missing keys in pca_index_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0dc50-34a8-46f2-b2fc-5da145595cc3",
   "metadata": {},
   "source": [
    "We run a suite of diagnostic tests on the 'Cumulative_Workload_Index_w' to ensure it captures the intended variance, while verifying that the index remains consistent with the underlying offensive, defensive, and special teams workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f4399-b8fc-4000-8615-0ef3c0389d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(con.execute(\"DESCRIBE team_week_panel\").df().tail(25))\n",
    "\n",
    "diag = con.execute(\"\"\"\n",
    "SELECT\n",
    "  AVG(Cumulative_Workload_Index_w) AS mean_index,\n",
    "  STDDEV_POP(Cumulative_Workload_Index_w) AS sd_index,\n",
    "  MIN(Cumulative_Workload_Index_w) AS min_index,\n",
    "  MAX(Cumulative_Workload_Index_w) AS max_index\n",
    "FROM team_week_panel\n",
    "\"\"\").df()\n",
    "\n",
    "print(diag)\n",
    "\n",
    "corrs = con.execute(\"\"\"\n",
    "SELECT\n",
    "  CORR(Cumulative_Workload_Index_w, cum_total_snaps_w) AS corr_total_snaps,\n",
    "  CORR(Cumulative_Workload_Index_w, cum_ST_Load_w) AS corr_st_load,\n",
    "  CORR(Cumulative_Workload_Index_w, cum_rest_deficit_days_w) AS corr_rest_deficit\n",
    "FROM team_week_panel\n",
    "\"\"\").df()\n",
    "\n",
    "print(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a530a53-52b2-44f2-8128-324293892b7d",
   "metadata": {},
   "source": [
    "We build a second PCA index using the per-game cumulative columns to isolate intensity from season duration, and also ensure the logic only executes if every required rate column is physically available in the panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09ca9d-83d1-47a1-a6c1-337712ab661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_inputs = [\n",
    "    \"cum_off_snaps_pg_w\",\n",
    "    \"cum_def_snaps_pg_w\",\n",
    "    \"cum_ST_Load_pg_w\",\n",
    "    \"cum_total_snaps_pg_w\",\n",
    "    \"cum_rest_deficit_days_pg_w\",\n",
    "]\n",
    "\n",
    "cols_now = _existing_cols(\"team_week_panel\")\n",
    "missing_pg = [c for c in pg_inputs if c not in cols_now]\n",
    "\n",
    "print(\"Missing per game inputs\", missing_pg)\n",
    "\n",
    "if missing_pg:\n",
    "    print(\"Skipping per game PCA index because required per game inputs are not all present\")\n",
    "else:\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    df_pg = con.execute(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "          season,\n",
    "          week,\n",
    "          {TEAM_ABBR_COL} AS team_key,\n",
    "          {\", \".join(pg_inputs)}\n",
    "        FROM team_week_panel\n",
    "        \"\"\"\n",
    "    ).df()\n",
    "\n",
    "    if df_pg.duplicated(subset=[\"season\", \"week\", \"team_key\"]).any():\n",
    "        n_dup = int(df_pg.duplicated(subset=[\"season\", \"week\", \"team_key\"]).sum())\n",
    "        raise RuntimeError(f\"Duplicate keys found in per game extract, duplicates {n_dup}\")\n",
    "\n",
    "    for c in pg_inputs:\n",
    "        df_pg[c] = pd.to_numeric(df_pg[c], errors=\"coerce\")\n",
    "\n",
    "    missing_rates_pg = df_pg[pg_inputs].isna().mean().sort_values(ascending=False)\n",
    "    print(\"Missing rate per per game input\")\n",
    "    print(missing_rates_pg)\n",
    "\n",
    "    if float(missing_rates_pg.max()) > 0.0:\n",
    "        bad = missing_rates_pg[missing_rates_pg > 0.0].index.tolist()\n",
    "        raise RuntimeError(\n",
    "            \"Step 8 cannot proceed because per game PCA inputs contain nulls, null columns are \" + \", \".join(bad)\n",
    "        )\n",
    "\n",
    "    means_pg = df_pg[pg_inputs].mean(axis=0)\n",
    "    sds_pg = df_pg[pg_inputs].std(axis=0, ddof=0)\n",
    "\n",
    "    zero_sd_pg = [c for c in pg_inputs if float(sds_pg[c]) == 0.0 or np.isclose(float(sds_pg[c]), 0.0)]\n",
    "    use_pg = [c for c in pg_inputs if c not in zero_sd_pg]\n",
    "\n",
    "    if len(use_pg) < 3:\n",
    "        raise RuntimeError(\"Too few usable per game inputs for PCA after dropping zero variance columns\")\n",
    "\n",
    "    Zpg = (df_pg[use_pg] - means_pg[use_pg]) / sds_pg[use_pg]\n",
    "    Zpg = Zpg.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "    pca_pg = PCA(n_components=1, svd_solver=\"full\")\n",
    "    pc1_pg = pca_pg.fit_transform(Zpg.values).reshape(-1)\n",
    "\n",
    "    explained_pg = float(pca_pg.explained_variance_ratio_[0])\n",
    "\n",
    "    anchor_pg = df_pg[\"cum_total_snaps_pg_w\"].values.astype(float)\n",
    "    corr_pg_before = float(np.corrcoef(pc1_pg, anchor_pg)[0, 1])\n",
    "\n",
    "    flip_pg = 1.0\n",
    "    if np.isfinite(corr_pg_before) and corr_pg_before < 0:\n",
    "        flip_pg = -1.0\n",
    "\n",
    "    pc1_pg_oriented = pc1_pg * flip_pg\n",
    "\n",
    "    print(\"Per game explained variance ratio PC1\", explained_pg)\n",
    "    print(\"Per game correlation with cum_total_snaps_pg_w before flip\", corr_pg_before)\n",
    "    print(\"Per game flip applied\", flip_pg)\n",
    "\n",
    "    df_pg_index = df_pg[[\"season\", \"week\", \"team_key\"]].copy()\n",
    "    df_pg_index[\"Cumulative_Workload_Index_pg_w\"] = pc1_pg_oriented.astype(float)\n",
    "\n",
    "    df_pg_index_for_db = df_pg_index.rename(columns={\"team_key\": TEAM_ABBR_COL})\n",
    "    con.register(\"pca_pg_index_df\", df_pg_index_for_db)\n",
    "\n",
    "    con.execute(\"DROP TABLE IF EXISTS pca_cum_workload_index_pg_tmp\")\n",
    "    con.execute(\"\"\"\n",
    "    CREATE TABLE pca_cum_workload_index_pg_tmp AS\n",
    "    SELECT * FROM pca_pg_index_df\n",
    "    \"\"\")\n",
    "\n",
    "    star = _star_excluding(\"team_week_panel\", \"p\", [\"Cumulative_Workload_Index_pg_w\"])\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE team_week_panel AS\n",
    "    SELECT\n",
    "      {star},\n",
    "      i.Cumulative_Workload_Index_pg_w\n",
    "    FROM team_week_panel p\n",
    "    LEFT JOIN pca_cum_workload_index_pg_tmp i\n",
    "    USING (season, week, {TEAM_ABBR_COL})\n",
    "    \"\"\")\n",
    "\n",
    "    nulls_pg = con.execute(\"\"\"\n",
    "    SELECT\n",
    "      SUM(CASE WHEN Cumulative_Workload_Index_pg_w IS NULL THEN 1 ELSE 0 END) AS n_null\n",
    "    FROM team_week_panel\n",
    "    \"\"\").df()[\"n_null\"].iloc[0]\n",
    "\n",
    "    print(\"Per game index null rows\", nulls_pg)\n",
    "\n",
    "    if nulls_pg != 0:\n",
    "        raise RuntimeError(\"Per game index has nulls after join, investigate missing keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336728d4-0ea2-4e85-b227-664b5d4330ca",
   "metadata": {},
   "source": [
    "Quick sanity check to confirm that the row count remains stable to ensure no records were lost or duplicated during the table join, while ensuring that the newly generated workload controls are populated for every team-week to prevent sample size attrition in the upcoming regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639338ef-057b-4e50-aaf1-bbab6aa26a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS rows,\n",
    "  SUM(CASE WHEN Cumulative_Workload_Index_w IS NULL THEN 1 ELSE 0 END) AS null_main,\n",
    "  SUM(CASE WHEN Cumulative_Workload_Index_pg_w IS NULL THEN 1 ELSE 0 END) AS null_pg\n",
    "FROM team_week_panel\n",
    "\"\"\").df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
