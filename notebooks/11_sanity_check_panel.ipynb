{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda5558b-48ad-4083-9305-ad26029016ad",
   "metadata": {},
   "source": [
    "We initialize Python imports and opens a DuckDB connection that every later cell reuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9c8ea-65f5-4dac-9442-66b700acf7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "\n",
    "print(\"cwd\", Path().resolve())\n",
    "\n",
    "candidates = []\n",
    "search_roots = [\n",
    "    Path(\".\"),\n",
    "    Path(\"..\"),\n",
    "    Path(\"./data\"),\n",
    "    Path(\"../data\"),\n",
    "    Path(\"../../data\"),\n",
    "]\n",
    "for root in search_roots:\n",
    "    if root.exists():\n",
    "        candidates.extend(list(root.glob(\"*.duckdb\")))\n",
    "        candidates.extend(list(root.glob(\"**/*.duckdb\")))\n",
    "\n",
    "seen = set()\n",
    "duckdb_files = []\n",
    "for f in candidates:\n",
    "    fp = str(f.resolve())\n",
    "    if fp not in seen:\n",
    "        seen.add(fp)\n",
    "        duckdb_files.append(f.resolve())\n",
    "\n",
    "print(\"duckdb files found\")\n",
    "for i, f in enumerate(duckdb_files[:25]):\n",
    "    print(i, f)\n",
    "\n",
    "db_file = None\n",
    "for f in duckdb_files:\n",
    "    if f.name == \"nflpa.duckdb\":\n",
    "        db_file = f\n",
    "        break\n",
    "\n",
    "if db_file is None and duckdb_files:\n",
    "    db_file = duckdb_files[0]\n",
    "\n",
    "if db_file is None:\n",
    "    raise RuntimeError(\"No duckdb file found near this notebook, rerun notebook 02 or check where you stored the database file\")\n",
    "\n",
    "con = duckdb.connect(str(db_file))\n",
    "print(\"connected db\", db_file)\n",
    "\n",
    "tables = con.execute(\"SHOW TABLES\").df()\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de6860-e756-42a7-91ef-4d08baad2315",
   "metadata": {},
   "source": [
    "We define small helpers for column discovery and strict required column checks so later sanity checks fail early if an upstream notebook was skipped or a column name drifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b309d3-3573-4423-b2de-ef1bf1aa62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SEASON_COL = \"season\"\n",
    "WEEK_COL = \"week\"\n",
    "TEAM_COL = \"team\"\n",
    "PANEL_TABLE = \"team_week_panel\"\n",
    "\n",
    "def _existing_cols(table_name: str) -> list[str]:\n",
    "    return con.execute(f\"DESCRIBE {table_name}\").df()[\"column_name\"].tolist()\n",
    "\n",
    "def _require_cols(table_name: str, required: list[str]) -> None:\n",
    "    cols = set(_existing_cols(table_name))\n",
    "    missing = [c for c in required if c not in cols]\n",
    "    print(\"Missing required columns\", missing)\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing columns in {table_name}, rerun earlier notebooks, missing, {missing}\")\n",
    "\n",
    "def _cols_matching(prefix: str = \"\", contains: str = \"\", suffix: str = \"\") -> list[str]:\n",
    "    cols = _existing_cols(PANEL_TABLE)\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        if prefix and not c.startswith(prefix):\n",
    "            continue\n",
    "        if contains and contains not in c:\n",
    "            continue\n",
    "        if suffix and not c.endswith(suffix):\n",
    "            continue\n",
    "        out.append(c)\n",
    "    return out\n",
    "\n",
    "print(\"team week panel columns\", len(_existing_cols(PANEL_TABLE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6106b1d-c40e-4688-b33b-7292d39b2598",
   "metadata": {},
   "source": [
    "Quick sanity check to confirm that the wide-format structure correctly reflects the expected mix of injury, workload, and control features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2164ad5-8b80-4a93-854e-4fc1029e408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"DESCRIBE team_week_panel\").df().head(30)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "SELECT\n",
    "  column_name,\n",
    "  column_type\n",
    "FROM (DESCRIBE team_week_panel)\n",
    "ORDER BY column_name\n",
    "LIMIT 30\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d77403-8c69-4cb9-ba0c-39e0372f43c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7d7eb-5482-47f6-9222-29816bc98a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = con.execute(\"DESCRIBE team_week_panel\").df()\n",
    "df_b = con.execute(\"\"\"\n",
    "SELECT column_name, column_type\n",
    "FROM (DESCRIBE team_week_panel)\n",
    "ORDER BY column_name\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"rows describe\", len(df_a))\n",
    "print(\"rows sorted\", len(df_b))\n",
    "\n",
    "dupe_names = df_a[\"column_name\"].value_counts()\n",
    "dupe_names = dupe_names[dupe_names > 1]\n",
    "print(\"duplicate column names\", dupe_names.to_dict())\n",
    "\n",
    "mismatch = (\n",
    "    df_a[[\"column_name\", \"column_type\"]]\n",
    "    .drop_duplicates()\n",
    "    .merge(\n",
    "        df_b[[\"column_name\", \"column_type\"]].drop_duplicates(),\n",
    "        on=[\"column_name\", \"column_type\"],\n",
    "        how=\"outer\",\n",
    "        indicator=True,\n",
    "    )\n",
    ")\n",
    "mismatch = mismatch[mismatch[\"_merge\"] != \"both\"]\n",
    "mismatch.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdaacaf-de71-48e5-9220-6e0cd13e71e6",
   "metadata": {},
   "source": [
    "Quick sanity check to confirm that 'team_week_panel' exists in the connected database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d4172-96ca-496c-bc68-2a18e0884a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN name = 'team_week_panel' THEN 1 ELSE 0 END) AS has_team_week_panel\n",
    "FROM (SHOW TABLES)\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f934e02-3294-48f7-b2f9-4f43c20e1aa7",
   "metadata": {},
   "source": [
    "Quick sanity check to confirm that 'team_week_panel' has no duplicate season-week-team rows and that the key count equals the row count to ensure that the final modeling table is a perfectly unique panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63646b74-e77a-439a-9218-b7e176d0ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS rows,\n",
    "  COUNT(DISTINCT season || '-' || week || '-' || team) AS distinct_keys\n",
    "FROM team_week_panel\n",
    "\"\"\").df()\n",
    "\n",
    "dups = con.execute(\"\"\"\n",
    "SELECT\n",
    "  season,\n",
    "  week,\n",
    "  team,\n",
    "  COUNT(*) AS n\n",
    "FROM team_week_panel\n",
    "GROUP BY 1,2,3\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY n DESC, season, week, team\n",
    "LIMIT 50\n",
    "\"\"\").df()\n",
    "\n",
    "dups\n",
    "\n",
    "if len(dups) > 0:\n",
    "    raise RuntimeError(\"Duplicate season week team rows exist in team_week_panel, investigate joins before proceeding\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
